# Architectures and layers
lstm_architecture.dropout_rate = 0.15
lstm_architecture.units = 64
gru_architecture.dropout_rate = 0.15
gru_architecture.units = 64
rnn_architecture.dropout_rate = 0.15
rnn_architecture.units = 64
bidi_lstm_architecture.dropout_rate = 0.30
bidi_lstm_architecture.units = 64

# Training
Trainer.total_steps = 1e3
Trainer.log_interval = 1e1
Trainer.ckpt_interval = 1e2
Trainer.learning_rate = 1e-4
Trainer.class_weight = [1., 1.06206897, 1.15789474, 1.01986755, 0.91666667, 0.93902439, 11.84615385, 15.4, 9.625,11., 7.7, 11.]

training.model_name = "LSTM_model"

# Evaluation
evaluation.model_name = "LSTM_model"

# Folder/Path generation
gen_run_folder.path_model_id = 'GRU_model'
gen_run_folder.new_model = True

# Input pipeline
load.name = 'hapt'
load.data_dir =  '/home/data' # For execution on ISS Server
#load.data_dir = 'C:/TS/Bildung/EIM/Semester_3/Deep_Learning_Lab/04_Datasets/HAPT_dataset' # For local execution on Tim's computer
load.tfrecord_files_exist = False # If false, TFRecord file are created for selected window size and window shift
load.window_size = 250
load.window_shift = 125
load.batch_size = 64
create_tfrecord_files.balance = False

# Visualization
visualization.range_time = [1000,20000]

# Tuning
tune.key = '8478ddb0f2c0978283abcb1e18db08bebd904d3f'

train_tune.tune = True
train_tune.evaluate = False

tune.parameters_dict = {
        'steps': {
            'min': 500,
            'max': 5000
        },
        'lr_rate': {
            'min': 0.00001,
            'max': 0.001
        },
        'drop_rate': {
            'min': 0.1,
            'max': 0.9
        },
        'model': {
            'values': ["LSTM_model", "GRU_model", "bidi_LSTM_model"]
        },
        'window_size': {
            'values': [125, 250, 375]
        },
        'window_shift': {
            'values': [50, 75, 100, 125]
        },
        'batch_size': {
            'values': [8, 16, 32, 64, 128, 256]
        },
        'units': {
            'values': [32, 64, 128]
        }
    }